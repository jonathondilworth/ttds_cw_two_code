{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_lookup = {\n",
    "    'Autos & Vehicles':'1',\n",
    "    'Comedy':'2',\n",
    "    'Education':'3',\n",
    "    'Entertainment':'4',\n",
    "    'Film & Animation':'5',\n",
    "    'Gaming':'6',\n",
    "    'Howto & Style':'7',\n",
    "    'Music':'8',\n",
    "    'News & Politics':'9',\n",
    "    'Nonprofits & Activism':'10',\n",
    "    'Pets & Animals':'11',\n",
    "    'Science & Technology':'12',\n",
    "    'Sports':'13',\n",
    "    'Travel & Events':'14'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_link(input_string):\n",
    "    return re.sub(r\"http\\S+\", \"\", input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_spec_chars(input_string):\n",
    "    return re.sub(r\"[^\\w# @_]\", \"\", input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_windows_file(filepath, encoding='cp1252'):\n",
    "    with codecs.open(filepath, 'r', encoding) as file:\n",
    "        return list(filter(str.strip, file.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweet_features_and_preprocess(data_set):\n",
    "    individual_features = [tweet.split(\"\\t\") for tweet in data_set]\n",
    "    for tweet in individual_features:\n",
    "        tweet[1] = ps.stem(remove_spec_chars(remove_link(tweet[1].lower())))\n",
    "    return individual_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_tweets = read_windows_file('../Tweets.14cat.train')\n",
    "testing_tweets = read_windows_file('../Tweets.14cat.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of tweets (string), no id, no label, links and special characters have been removed\n",
    "# training_tweets_text = [remove_spec_chars(remove_link((tweet.split(\"\\t\"))[1].lower())) for tweet in training_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same as above, but includes stemming\n",
    "training_tweets_text = [ps.stem(remove_spec_chars(remove_link((tweet.split(\"\\t\"))[1].lower()))) for tweet in training_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_words = set(word_tokenize(' '.join(training_tweets_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_without_stopwords = unique_words.difference(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the set of features (BoW) will consist of integers (1->N) mapping each token (word) in the training set\n",
    "feature_set = dict(enumerate(list(unique_words_without_stopwords), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write our feature set to disk\n",
    "with codecs.open('./feats.dic', 'w', 'UTF-8') as file:\n",
    "    for attribute, value in feature_set.items():\n",
    "        file.write(\"{}\\t{}\\n\".format(attribute, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our application requires features to be attribute value pairs, where the attribute is the word & value is the int\n",
    "useable_feature_set = {v:k for k,v in feature_set.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_features = get_tweet_features_and_preprocess(training_tweets)\n",
    "testing_features = get_tweet_features_and_preprocess(testing_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: refactor into 3 seperate functions? write label, write body and write comment?\n",
    "# TODO: consider passing useable_feature_set, label_lookup as parameters, instead of globals\n",
    "# globals R bad\n",
    "# useable_feature_set should really be variable..\n",
    "\n",
    "# recall, we can't actually have to remove stopwords, it would improve the training time, but it wouldn't make\n",
    "# a difference to accuracy, as the stopwords don't exist in our feats.dict file anyway..\n",
    "# this means the loop would just 'continue' when it encountered a stop word.\n",
    "# However, if we were doing this at scale, we would probably want to remove these to improve training times..\n",
    "\n",
    "def write_ml_ready_file(filepath, feature_data, encoding=\"UTF-8\"):\n",
    "    with codecs.open(filepath, 'w', encoding) as file:\n",
    "        for tweet in feature_data:\n",
    "            try:\n",
    "                file.write(label_lookup[tweet[2].strip()] + \" \")\n",
    "                feature_body = []\n",
    "                for tweet_text in word_tokenize(tweet[1]):\n",
    "                    try:\n",
    "                        feature_body.append(useable_feature_set[tweet_text.strip()])\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                feature_body = set(feature_body)\n",
    "                feature_body = sorted(feature_body)\n",
    "                file.write(' '.join([str(feature) + \":1\" for feature in feature_body]) + \" \")\n",
    "                file.write(\"#\" + str(tweet[0]) + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(\"Whoops!\")\n",
    "                print(e)\n",
    "                file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_ml_ready_file('./feats.train', training_features)\n",
    "write_ml_ready_file('./feats.test', testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply stopping and stemming in preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try applying lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plan Moving Forwards\n",
    "# 1) improve the accuracy of the SVM as much as possible, ideally I'd like to hit 70% +\n",
    "# 2) switch architecture...\n",
    "# 2.1) we could try and use the SVM with word embeddings, or..\n",
    "# 2.2) we could switch over to word embeddings + LSTM NN..\n",
    "#    ) Reference: Pycon Ireland video thingy, Embedding Layer, LSTM, Dense + Dropout (we'd probably use a softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I'm going to need a function that I can pass a word vector to, and it can generate word embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# then I'm going to need to use Keras, LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
